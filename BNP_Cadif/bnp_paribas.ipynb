{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def binarize(columnName, df, features=None):\n",
    "    df[columnName] = df[columnName].astype(str)\n",
    "    if features is None:\n",
    "        features = np.unique(df[columnName].values)\n",
    "    for x in features:\n",
    "        df[columnName+'_' + x] = df[columnName].map(lambda y: 1 if y == x else 0)\n",
    "    return df, features\n",
    "\n",
    "\n",
    "def add_bernulli(train, test, features):\n",
    "    for col in features:\n",
    "        train, binfeatures = binarize(col, train)\n",
    "        test, _ = binarize(col, test, binfeatures)\n",
    "        nb = BernoulliNB()\n",
    "        nb.fit(train[col + '_' + binfeatures].values, y_train)\n",
    "        train['naive_' + col] = nb.predict_proba(train[col + '_' + binfeatures].values)[:, 1]\n",
    "        test['naive_' + col] = nb.predict_proba(test[col + '_' + binfeatures].values)[:, 1]\n",
    "        train.drop(col + '_' + binfeatures, inplace=True, axis=1)\n",
    "        test.drop(col + '_' + binfeatures, inplace=True, axis=1)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def factorize(train, test):\n",
    "    for (train_name, train_series), (test_name, test_series) in zip(train.iteritems(), test.iteritems()):\n",
    "        if train_series.dtype == 'O':\n",
    "            train[train_name], tmp_indexer = pd.factorize(train[train_name])\n",
    "            test[test_name] = tmp_indexer.get_indexer(test[test_name])\n",
    "        else:\n",
    "            tmp_len = len(train[train_series.isnull()])\n",
    "            if tmp_len > 0:\n",
    "                train.loc[train_series.isnull(), train_name] = -2\n",
    "            tmp_len = len(test[test_series.isnull()])\n",
    "            if tmp_len > 0:\n",
    "                test.loc[test_series.isnull(), test_name] = -2\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def stacking(X_train, X_test, y_train, skf, clfs):\n",
    "    meta_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "    meta_test  = np.zeros((X_test.shape[0],  len(clfs)))\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        print('Clf', j+1)\n",
    "        meta_test_j = np.zeros((X_test.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print('Fold', i+1)\n",
    "            X_tr = X_train[train]\n",
    "            y_tr = y_train[train]\n",
    "            X_ts = X_train[test]\n",
    "            y_ts = y_train[test]\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_submission = clf.predict_proba(X_ts)[:, 1]\n",
    "            meta_train[test, j] = y_submission\n",
    "            meta_test_j[:, i] = clf.predict_proba(X_test)[:, 1]\n",
    "        meta_test[:, j] = meta_test_j.mean(1)\n",
    "        gc.collect()\n",
    "        \n",
    "    return meta_train, meta_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class addNearestNeighbourLinearFeatures:\n",
    "    \n",
    "    def __init__(self, n_neighbours=1, max_elts=None, verbose=True, random_state=None):\n",
    "        self.rnd = random_state\n",
    "        self.n = n_neighbours\n",
    "        self.max_elts = max_elts\n",
    "        self.verbose = verbose\n",
    "        self.neighbours = []\n",
    "        self.clfs = []\n",
    "        \n",
    "    def fit(self,train, y):\n",
    "        if self.rnd != None:\n",
    "            random.seed(rnd)\n",
    "        if self.max_elts == None:\n",
    "            self.max_elts = len(train.columns)\n",
    "        list_vars = list(train.columns)\n",
    "        random.shuffle(list_vars)\n",
    "        \n",
    "        lastscores = np.zeros(self.n) + 1e15\n",
    "\n",
    "        for elt in list_vars[:self.n]:\n",
    "            self.neighbours.append([elt])\n",
    "        list_vars = list_vars[self.n:]\n",
    "        \n",
    "        for elt in list_vars:\n",
    "            indice = 0\n",
    "            scores = []\n",
    "            for elt2 in self.neighbours:\n",
    "                if len(elt2) < self.max_elts:\n",
    "                    clf = LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n",
    "                    clf.fit(train[elt2 + [elt]], y)\n",
    "                    scores.append(log_loss(y,clf.predict(train[elt2 + [elt]])))\n",
    "                    indice = indice + 1\n",
    "                else:\n",
    "                    scores.append(lastscores[indice])\n",
    "                    indice = indice + 1\n",
    "            gains = lastscores - scores\n",
    "            if gains.max() > 0:\n",
    "                temp = gains.argmax()\n",
    "                lastscores[temp] = scores[temp]\n",
    "                self.neighbours[temp].append(elt)\n",
    "\n",
    "        indice = 0\n",
    "        for elt in self.neighbours:\n",
    "            clf = LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n",
    "            clf.fit(train[elt], y)\n",
    "            self.clfs.append(clf)\n",
    "            if self.verbose:\n",
    "                print(indice, lastscores[indice], elt)\n",
    "            indice = indice + 1\n",
    "                    \n",
    "    def transform(self, train):\n",
    "        indice = 0\n",
    "        for elt in self.neighbours:\n",
    "            train['_'.join(pd.Series(elt).sort_values().values)] = self.clfs[indice].predict(train[elt])\n",
    "            indice = indice + 1\n",
    "        return train\n",
    "    \n",
    "    def fit_transform(self, train, y):\n",
    "        self.fit(train, y)\n",
    "        return self.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerMixin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-16a2c18abe35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAddfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_elts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_neighbours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_elts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_elts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerMixin' is not defined"
     ]
    }
   ],
   "source": [
    "class Addfeatures(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self, n_neighbours=1, max_elts=None):\n",
    "        self.n = n_neighbours\n",
    "        self.max_elts = max_elts\n",
    "        self.neighbours = []\n",
    "        self.clfs = []\n",
    "        \n",
    "    def fit(self,X, y):\n",
    "        if self.max_elts == None:\n",
    "            self.max_elts = len(X.columns)\n",
    "        list_vars = list(X.columns)\n",
    "        random.shuffle(list_vars)\n",
    "        \n",
    "        lastscores = np.zeros(self.n) + 1e15\n",
    "\n",
    "        for elt in list_vars[:self.n]:\n",
    "            self.neighbours.append([elt])\n",
    "        list_vars = list_vars[self.n:]\n",
    "        \n",
    "        for elt in list_vars:\n",
    "            indice = 0\n",
    "            scores = []\n",
    "            for elt2 in self.neighbours:\n",
    "                if len(elt2) < self.max_elts:\n",
    "                    clf = LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n",
    "                    clf.fit(train[elt2 + [elt]], y)\n",
    "                    scores.append(log_loss(y,clf.predict(train[elt2 + [elt]])))\n",
    "                    indice = indice + 1\n",
    "                else:\n",
    "                    scores.append(lastscores[indice])\n",
    "                    indice = indice + 1\n",
    "            gains = lastscores - scores\n",
    "            if gains.max() > 0:\n",
    "                temp = gains.argmax()\n",
    "                lastscores[temp] = scores[temp]\n",
    "                self.neighbours[temp].append(elt)\n",
    "\n",
    "        indice = 0\n",
    "        for elt in self.neighbours:\n",
    "            clf = LinearRegression(fit_intercept=False, normalize=True, copy_X=True, n_jobs=-1) \n",
    "            clf.fit(train[elt], y)\n",
    "            self.clfs.append(clf)\n",
    "            indice = indice + 1\n",
    "                    \n",
    "    def transform(self, train):\n",
    "        indice = 0\n",
    "        for elt in self.neighbours:\n",
    "            train['_'.join(pd.Series(elt).sort_values().values)] = self.clfs[indice].predict(train[elt])\n",
    "            indice = indice + 1\n",
    "        return train\n",
    "    \n",
    "    def fit_transform(self, train, y):\n",
    "        self.fit(train, y)\n",
    "        return self.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(train, test):\n",
    "    \n",
    "    drop_columns = ['v8', 'v23', 'v25', 'v31', 'v36', 'v37', 'v46', 'v51', 'v53', 'v54', 'v63', 'v73', 'v75',\n",
    "                    'v79', 'v81', 'v82', 'v89', 'v92', 'v95', 'v105', 'v107', 'v108', 'v109', 'v110', 'v116', 'v117',\n",
    "                    'v118', 'v119', 'v123', 'v124', 'v128']\n",
    "    train.drop(drop_columns, axis=1, inplace=True)\n",
    "    test.drop(drop_columns, axis=1, inplace=True)\n",
    "    \n",
    "    naive_vars = ['v24', 'v112', 'v30', 'v91', 'v56', 'v74', 'v125', 'v71', 'v113', 'v47', 'v3', 'v66']\n",
    "    \n",
    "    train, test = factorize(train, test)\n",
    "    train, test = add_bernulli(train, test, naive_vars)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(train, test):\n",
    "    trainids = train.ID.values\n",
    "    testids = test.ID.values\n",
    "    targets = train['target'].values\n",
    "    tokeep = [ 'v3', 'v10', 'v12', 'v14', 'v21', 'v22', 'v24', 'v30', 'v31', 'v34', 'v38', 'v40',\n",
    "              'v50', 'v52', 'v56', 'v62', 'v66', 'v71', 'v72', 'v74', 'v75', 'v79', 'v91', 'v47',  \n",
    "              'v112', 'v113', 'v114', 'v125', 'v129']\n",
    "    features = train.columns[2:]\n",
    "    todrop = list(set(features).difference(tokeep))\n",
    "    train.drop(todrop, inplace=True, axis=1)\n",
    "    test.drop(todrop, inplace=True, axis=1)\n",
    "    features = train.columns[2:]\n",
    "    for col in features:\n",
    "        if((train[col].dtype == 'object')):\n",
    "            train.loc[~train[col].isin(test[col]), col] = 'Orphans'\n",
    "            test.loc[~test[col].isin(train[col]), col] = 'Orphans'\n",
    "            train[col].fillna('Missing', inplace=True)\n",
    "            test[col].fillna('Missing', inplace=True)\n",
    "            train[col], tmp_indexer = pd.factorize(train[col])\n",
    "            test[col] = tmp_indexer.get_indexer(test[col])\n",
    "            traincounts = train[col].value_counts().reset_index()\n",
    "            traincounts.rename(columns={'index': col, col: col+'_count'}, inplace=True)\n",
    "            traincounts = traincounts[traincounts[col+'_count'] >= 50]\n",
    "            g = train[[col, 'target']].copy().groupby(col).mean().reset_index()\n",
    "            g = g[g[col].isin(traincounts[col])]\n",
    "            g.rename(columns={'target': col+'_avg'}, inplace=True)\n",
    "            train = train.merge(g, how='left', on=col)\n",
    "            test = test.merge(g, how='left', on=col)\n",
    "            h = train[[col, 'target']].copy().groupby(col).std().reset_index()\n",
    "            h = h[h[col].isin(traincounts[col])]\n",
    "            h.rename(columns={'target': col+'_std'}, inplace=True)\n",
    "            train = train.merge(h, how='left', on=col)\n",
    "            test = test.merge(h, how='left', on=col)\n",
    "            train.drop(col, inplace=True, axis=1)\n",
    "            test.drop(col, inplace=True, axis=1)\n",
    "\n",
    "    features = train.columns[2:]\n",
    "    train.fillna(-1, inplace=True)\n",
    "    test.fillna(-1, inplace=True)\n",
    "    train[features] = train[features].astype(float)\n",
    "    test[features] = test[features].astype(float)\n",
    "    ss = StandardScaler()\n",
    "    train[features] = np.round(ss.fit_transform(train[features].values), 6)\n",
    "    test[features] = np.round(ss.transform(test[features].values), 6)\n",
    "    gptrain = pd.DataFrame()\n",
    "    gptest = pd.DataFrame()\n",
    "    gptrain.insert(0, 'ID', trainids)\n",
    "    gptest.insert(0, 'ID', testids)\n",
    "    gptrain = pd.merge(gptrain, train[list(['ID'])+list(features)], on='ID')\n",
    "    gptest = pd.merge(gptest, test[list(['ID'])+list(features)], on='ID')\n",
    "    gptrain['TARGET'] = targets\n",
    "    del train\n",
    "    del test\n",
    "    gc.collect()\n",
    "    \n",
    "    return gptrain, gptest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('train.csv')['target'].values\n",
    "cv = StratifiedKFold(y_train, n_folds=5, shuffle=True, random_state=115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking: 1 level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = list(StratifiedKFold(y_train, n_folds=5, shuffle=True, random_state=115))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "y_train = train['target'].values\n",
    "id_test = test['ID'].values\n",
    "\n",
    "train.drop(['ID', 'target'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test = preprocess_data(train, test)\n",
    "\n",
    "X_train_n = np.array(X_train, dtype=np.float32)\n",
    "X_test_n = np.array(X_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=50, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(X_train_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = pca.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=50, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 112)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 2\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=800, criterion='entropy', max_depth=37, max_features=25, \n",
    "                             min_samples_split=4, min_samples_leaf=2, n_jobs=-1, random_state=888),                  \n",
    "        XGBClassifier(n_estimators=600, learning_rate=0.03, max_depth=10, colsample_bytree=0.4, \n",
    "                      min_child_weight=1, seed=88888), \n",
    "        RandomForestClassifier(n_estimators=800, criterion='gini')]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_1 = pd.DataFrame(meta_train, index=X_train.index, columns=['base_et', 'base_xgb', 'base_rf'])\n",
    "meta_test_1 = pd.DataFrame(meta_test, index=X_test.index, columns=['base_et', 'base_xgb', 'base_rf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_n = ss.fit_transform(X_train_n)\n",
    "X_test_n = ss.transform(X_test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "clfs = [LogisticRegression(C=1.0, penalty='l2', n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_2 = pd.DataFrame(meta_train, index=X_train.index, columns=['base_lr'])\n",
    "meta_test_2 = pd.DataFrame(meta_test, index=X_test.index, columns=['base_lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encodered features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tu/miniconda3/envs/env36/lib/python3.6/site-packages/pandas/core/generic.py:3110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "values, counts = np.unique(X_train.v22, return_counts=True)\n",
    "counts = {x : y for x, y in zip(values, counts)}\n",
    "X_train.v22 = X_train.v22.apply(lambda x: x if counts.get(x, 0) > 50 else 0)\n",
    "X_test.v22 = X_test.v22.apply(lambda x: x if counts.get(x, 0) > 50 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_vars = ['v24', 'v112', 'v30', 'v91', 'v52', 'v56', 'v74', 'v125', 'v71', 'v113', 'v47', 'v3', 'v66']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_vars += ['v22']\n",
    "X_train = X_train[cat_vars]\n",
    "X_test = X_test[cat_vars]\n",
    "gc.collect()\n",
    "\n",
    "data = pd.concat((X_train, X_test), axis=0, ignore_index=True)\n",
    "for col in cat_vars:\n",
    "    data = pd.concat((data, pd.get_dummies(data[col], prefix=col)), axis=1)\n",
    "    data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "X_train_n = np.array(data, dtype=np.float32)[:X_train.shape[0]]\n",
    "X_test_n = np.array(data, dtype=np.float32)[X_train.shape[0]:]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 2\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 4\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=45, max_features=30, \n",
    "                              random_state=333, n_jobs=-1),\n",
    "        XGBClassifier(max_depth=8, learning_rate=0.05, n_estimators=600, min_child_weight=5, \n",
    "                      subsample=0.95, colsample_bytree=0.35, seed=901),\n",
    "        RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=50, max_features=20, \n",
    "                              random_state=333, n_jobs=-1),\n",
    "        LogisticRegression(C=0.05, penalty='l2', n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_3 = pd.DataFrame(meta_train, index=X_train.index, columns=['ohe_et', 'ohe_xgb', 'ohe_rf', 'ohe_lr'])\n",
    "meta_test_3 = pd.DataFrame(meta_test, index=X_test.index, columns=['ohe_et', 'ohe_xgb', 'ohe_rf', 'ohe_lr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Neighbors linear regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.drop(['ID', 'target'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "train, test = factorize(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add more 4 variables from columns v22, transform LabelEnconding values of columns into Unicode values\n",
    "train['v22-1'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "test['v22-1'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "train['v22-2'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "test['v22-2'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "train['v22-3'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "test['v22-3'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "train['v22-4'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "test['v22-4'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "\n",
    "drop_list=['v91','v1', 'v8', 'v10', 'v15', 'v17', 'v25', 'v29', 'v34', 'v41', \n",
    "           'v46', 'v54', 'v64', 'v67', 'v97', 'v105', 'v111', 'v122']\n",
    "\n",
    "train = train.drop(drop_list,axis=1).fillna(-2)\n",
    "test = test.drop(drop_list,axis=1).fillna(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd = 12\n",
    "random.seed(rnd)\n",
    "n_ft = 20 # Number of features to add\n",
    "max_elts = 3 # Maximum size of a group of linear features\n",
    "\n",
    "a = addNearestNeighbourLinearFeatures(n_neighbours=n_ft, max_elts=max_elts, verbose=False, random_state=rnd)\n",
    "a.fit(train, y_train)\n",
    "\n",
    "train = a.transform(train)\n",
    "test = a.transform(test)\n",
    "\n",
    "X_train_n = np.array(train, dtype=float)\n",
    "X_test_n = np.array(test, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v123_v68_v76</th>\n",
       "      <th>v20_v39_v98</th>\n",
       "      <th>v118_v17_v60</th>\n",
       "      <th>v130_v23_v51</th>\n",
       "      <th>v117_v16_v63</th>\n",
       "      <th>v102_v128_v5</th>\n",
       "      <th>v43_v67_v73</th>\n",
       "      <th>v35_v58_v75</th>\n",
       "      <th>v127_v129_v87</th>\n",
       "      <th>v116_v33_v42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.335739</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>0</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>0.503281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.692118</td>\n",
       "      <td>0.351290</td>\n",
       "      <td>0.330708</td>\n",
       "      <td>1.107842</td>\n",
       "      <td>0.584594</td>\n",
       "      <td>0.546231</td>\n",
       "      <td>0.947730</td>\n",
       "      <td>0.458984</td>\n",
       "      <td>0.432507</td>\n",
       "      <td>1.015836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>9.191265</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>2.301630</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>1.312910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501866</td>\n",
       "      <td>0.231017</td>\n",
       "      <td>0.668665</td>\n",
       "      <td>0.393885</td>\n",
       "      <td>0.597250</td>\n",
       "      <td>0.743152</td>\n",
       "      <td>0.619850</td>\n",
       "      <td>-0.103345</td>\n",
       "      <td>0.881238</td>\n",
       "      <td>0.655353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943877</td>\n",
       "      <td>5.310079</td>\n",
       "      <td>0</td>\n",
       "      <td>4.410969</td>\n",
       "      <td>5.326159</td>\n",
       "      <td>3.979592</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305699</td>\n",
       "      <td>-0.178190</td>\n",
       "      <td>0.034876</td>\n",
       "      <td>1.452979</td>\n",
       "      <td>0.311165</td>\n",
       "      <td>0.490311</td>\n",
       "      <td>0.314540</td>\n",
       "      <td>0.991893</td>\n",
       "      <td>0.917135</td>\n",
       "      <td>0.507833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.797415</td>\n",
       "      <td>8.304757</td>\n",
       "      <td>0</td>\n",
       "      <td>4.225930</td>\n",
       "      <td>11.627438</td>\n",
       "      <td>2.097700</td>\n",
       "      <td>1.987549</td>\n",
       "      <td>0.171947</td>\n",
       "      <td>8.965516</td>\n",
       "      <td>6.542669</td>\n",
       "      <td>...</td>\n",
       "      <td>1.036160</td>\n",
       "      <td>0.868730</td>\n",
       "      <td>0.745239</td>\n",
       "      <td>0.288920</td>\n",
       "      <td>0.431143</td>\n",
       "      <td>1.212713</td>\n",
       "      <td>1.028347</td>\n",
       "      <td>0.483741</td>\n",
       "      <td>0.848629</td>\n",
       "      <td>1.037825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>1.050328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501866</td>\n",
       "      <td>0.190606</td>\n",
       "      <td>0.668665</td>\n",
       "      <td>0.393885</td>\n",
       "      <td>0.043838</td>\n",
       "      <td>0.495921</td>\n",
       "      <td>0.619850</td>\n",
       "      <td>-0.103345</td>\n",
       "      <td>0.022069</td>\n",
       "      <td>0.655353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2  v3        v4         v5        v6        v7        v8  \\\n",
       "0  1.335739  8.727474   0  3.921026   7.915266  2.599278  3.176895  0.012941   \n",
       "1 -2.000000 -2.000000   0 -2.000000   9.191265 -2.000000 -2.000000  2.301630   \n",
       "2  0.943877  5.310079   0  4.410969   5.326159  3.979592  3.928571  0.019645   \n",
       "3  0.797415  8.304757   0  4.225930  11.627438  2.097700  1.987549  0.171947   \n",
       "4 -2.000000 -2.000000   0 -2.000000  -2.000000 -2.000000 -2.000000 -2.000000   \n",
       "\n",
       "          v9       v10      ...       v123_v68_v76  v20_v39_v98  v118_v17_v60  \\\n",
       "0   9.999999  0.503281      ...           0.692118     0.351290      0.330708   \n",
       "1  -2.000000  1.312910      ...           0.501866     0.231017      0.668665   \n",
       "2  12.666667  0.765864      ...           0.305699    -0.178190      0.034876   \n",
       "3   8.965516  6.542669      ...           1.036160     0.868730      0.745239   \n",
       "4  -2.000000  1.050328      ...           0.501866     0.190606      0.668665   \n",
       "\n",
       "   v130_v23_v51  v117_v16_v63  v102_v128_v5  v43_v67_v73  v35_v58_v75  \\\n",
       "0      1.107842      0.584594      0.546231     0.947730     0.458984   \n",
       "1      0.393885      0.597250      0.743152     0.619850    -0.103345   \n",
       "2      1.452979      0.311165      0.490311     0.314540     0.991893   \n",
       "3      0.288920      0.431143      1.212713     1.028347     0.483741   \n",
       "4      0.393885      0.043838      0.495921     0.619850    -0.103345   \n",
       "\n",
       "   v127_v129_v87  v116_v33_v42  \n",
       "0       0.432507      1.015836  \n",
       "1       0.881238      0.655353  \n",
       "2       0.917135      0.507833  \n",
       "3       0.848629      1.037825  \n",
       "4       0.022069      0.655353  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 2\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=31, max_features=50, \n",
    "                             min_samples_split=2, min_samples_leaf=2, random_state=610, n_jobs=-1),\n",
    "        XGBClassifier(n_estimators=350, learning_rate=0.05, max_depth=10, min_child_weight=5, \n",
    "                      subsample=1.0, colsample_bytree=0.4, seed=120),\n",
    "        RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=15, max_features=30, \n",
    "                            random_state=123, n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_4 = pd.DataFrame(meta_train, index=X_train.index, columns=['nnlr_et', 'nnlr_xgb', 'nnlr_rf'])\n",
    "meta_test_4 = pd.DataFrame(meta_test, index=X_test.index, columns=['nnlr_et', 'nnlr_xgb', 'nnlr_rf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another one script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train, test = prepare_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop(['ID', 'TARGET'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "X_train_n = np.array(train, dtype=np.float32)\n",
    "X_test_n = np.array(test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 2\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=19, max_features=20, \n",
    "                              random_state=671, n_jobs=-1),\n",
    "        XGBClassifier(n_estimators=280, learning_rate=0.05, max_depth=8, min_child_weight=5, \n",
    "                      subsample=0.9, colsample_bytree=0.45, seed=511),\n",
    "        RandomForestClassifier(n_estimators=500, criterion='gini', max_depth=12, max_features=15,\n",
    "                           random_state=181, n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_5 = pd.DataFrame(meta_train, index=X_train.index, columns=['small_et', 'small_xgb', 'small_rf'])\n",
    "meta_test_5 = pd.DataFrame(meta_test, index=X_test.index, columns=['small_et', 'small_xgb', 'small_rf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train_level1 = pd.concat((meta_train_1, meta_train_2, meta_train_3, meta_train_4, meta_train_5), axis=1)\n",
    "meta_test_level1 = pd.concat((meta_test_1, meta_test_2, meta_test_3, meta_test_4, meta_test_5), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train_level1.to_csv('meta_train_level1.csv', index=False)\n",
    "meta_test_level1.to_csv('meta_test_level1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train_level1 = pd.read_csv('meta_train_level1.csv')\n",
    "meta_test_level1 = pd.read_csv('meta_test_level1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking: 2 level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "y_train = train['target'].values\n",
    "id_test = test['ID'].values\n",
    "\n",
    "train.drop(['ID', 'target'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test = preprocess_data(train, test)\n",
    "\n",
    "X_train_2 = pd.concat((X_train, meta_train_level1), axis=1)\n",
    "X_test_2 = pd.concat((X_test, meta_test_level1), axis=1)\n",
    "\n",
    "X_train_n = np.array(X_train_2, dtype=np.float32)\n",
    "X_test_n = np.array(X_test_2, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 2\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=1000, max_features=25, criterion='entropy', min_samples_split=2, \n",
    "                             max_depth=36, min_samples_leaf=2, n_jobs=-1, random_state=888),\n",
    "        XGBClassifier(n_estimators=600, learning_rate=0.03, max_depth=10, colsample_bytree=0.4, \n",
    "                      min_child_weight=1, seed=129)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_0 = pd.DataFrame(meta_train, index=X_train.index, columns=['main_et', 'main_xgb'])\n",
    "meta_test_0 = pd.DataFrame(meta_test, index=X_test.index, columns=['main_et', 'main_xgb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Neighbors linear regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.drop(['ID', 'target'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)\n",
    "\n",
    "train, test = factorize(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['v22-1'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "test['v22-1'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "train['v22-2'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "test['v22-2'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "train['v22-3'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "test['v22-3'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "train['v22-4'] = train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "test['v22-4'] = test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "\n",
    "drop_list=['v91','v1', 'v8', 'v10', 'v15', 'v17', 'v25', 'v29', 'v34', 'v41', \n",
    "           'v46', 'v54', 'v64', 'v67', 'v97', 'v105', 'v111', 'v122']\n",
    "\n",
    "train = train.drop(drop_list,axis=1).fillna(-2)\n",
    "test = test.drop(drop_list,axis=1).fillna(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_vars = list(train.columns)\n",
    "random.shuffle(list_vars)\n",
    "neighbours = []\n",
    "max_elts = 3\n",
    "clfs = []\n",
    "scores = []\n",
    "\n",
    "lastscores = np.zeros(20) + 1e15\n",
    "\n",
    "for elt in list_vars[:20]:\n",
    "    neighbours.append([elt])\n",
    "list_vars = list_vars[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd = 12\n",
    "random.seed(rnd)\n",
    "n_ft = 20 # Number of features to add\n",
    "max_elts = 3 # Maximum size of a group of linear features\n",
    "\n",
    "a = addNearestNeighbourLinearFeatures(n_neighbours=n_ft, max_elts=max_elts, verbose=False, random_state=rnd)\n",
    "a.fit(train, y_train)\n",
    "\n",
    "train = a.transform(train)\n",
    "test = a.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2 = pd.concat((train, meta_train_level1), axis=1)\n",
    "X_test_2 = pd.concat((test, meta_test_level1), axis=1)\n",
    "\n",
    "X_train_n = np.array(X_train_2, dtype=np.float32)\n",
    "X_test_n = np.array(X_test_2, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=32, max_features=50, \n",
    "                             min_samples_split=2, min_samples_leaf=2, random_state=171, n_jobs=-1),\n",
    "        XGBClassifier(n_estimators=330, learning_rate=0.03, max_depth=8, colsample_bytree=0.45, \n",
    "                      min_child_weight=5, subsample=1.0, seed=666)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_00 = pd.DataFrame(meta_train, index=train.index, columns=['main_nnlr_et', 'main_nnlr_xgb'])\n",
    "meta_test_00 = pd.DataFrame(meta_test, index=test.index, columns=['main_nnlr_et', 'main_nnlr_xgb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train_00 = pd.read_csv('meta_train_00.csv')\n",
    "meta_test_00 = pd.read_csv('meta_test_00.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another one script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train, test = prepare_data(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.drop(['ID', 'TARGET'], axis=1, inplace=True)\n",
    "test.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2 = pd.concat((train, meta_train_level1), axis=1)\n",
    "X_test_2 = pd.concat((test, meta_test_level1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_n = np.array(X_train_2, dtype=np.float32)\n",
    "X_test_n = np.array(X_test_2, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 2\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=500, criterion='entropy', max_depth=14, max_features=32, \n",
    "                             min_samples_split=2, min_samples_leaf=3, random_state=10, n_jobs=-1),\n",
    "        XGBClassifier(n_estimators=650, learning_rate=0.01, max_depth=7, colsample_bytree=0.4,\n",
    "              min_child_weight=5, subsample=1.0, seed=89)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_000 = pd.DataFrame(meta_train, index=train.index, columns=['main_small_et', 'main_small_xgb'])\n",
    "meta_test_000 = pd.DataFrame(meta_test, index=test.index, columns=['main_small_et', 'main_small_xgb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train_level2 = pd.concat((meta_train_0, meta_train_00, meta_train_000), axis=1)\n",
    "meta_test_level2 = pd.concat((meta_test_0, meta_test_00, meta_test_000), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train_level2.to_csv('meta_train_level2.csv', index=False)\n",
    "meta_test_level2.to_csv('meta_test_level2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_train_level2 = pd.read_csv('meta_train_level2.csv')\n",
    "meta_test_level2 = pd.read_csv('meta_test_level2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-199eb5ab5e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TARGET'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepare_data' is not defined"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_train, X_test = prepare_data(train, test)\n",
    "\n",
    "X_train.drop(['ID', 'TARGET'], axis=1, inplace=True)\n",
    "X_test.drop(['ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_test = test['ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_3 = pd.concat((X_train, meta_train_level1, meta_train_level2), axis=1)\n",
    "X_test_3 = pd.concat((X_test, meta_test_level1, meta_test_level2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-25f221ac3902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mid_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PredictedProb\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bnp_predict_6.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'id_test' is not defined"
     ]
    }
   ],
   "source": [
    "clf = CalibratedClassifierCV(ExtraTreesClassifier(n_estimators=2000, max_features=30, criterion='entropy', \n",
    "                                                  min_samples_split=2, max_depth=14, min_samples_leaf=2, \n",
    "                                                  n_jobs=-1, random_state=190), method='isotonic', cv= cv)\n",
    "\n",
    "clf.fit(X_train_3, y_train) \n",
    "y_pred = clf.predict_proba(X_test_3)[:, 1]\n",
    "\n",
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": y_pred}).to_csv('bnp_predict_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"ID\": id_test, \"PredictedProb\": y_pred}).to_csv('bnp_predict_6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
