{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = [c for c in train.columns if c not in ['id','target']]\n",
    "\n",
    "x1, x2, y1, y2 = model_selection.train_test_split(train[col], train['target'], test_size=0.25, random_state=99)\n",
    "x1 = x1.values.astype(np.float32)\n",
    "x2 = x2.values.astype(np.float32)\n",
    "y1 = y1.values.astype(np.int)\n",
    "y2 = y2.values.astype(np.int)\n",
    "xtest = test[col].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    "\n",
    "def gini_tf(pred, y):\n",
    "    return gini(y, pred) / gini(y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpy7f1dawf\n",
      "INFO:tensorflow:Using config: {'_environment': 'local', '_model_dir': '/tmp/tmpy7f1dawf', '_save_checkpoints_steps': None, '_master': '', '_task_type': None, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_num_ps_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1ed69d6748>, '_keep_checkpoint_every_n_hours': 10000, '_num_worker_replicas': 0, '_evaluation_master': '', '_session_config': None, '_is_chief': True}\n",
      "WARNING:tensorflow:From /home/tu/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpy7f1dawf/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.217901, step = 1\n",
      "INFO:tensorflow:global_step/sec: 29.7195\n",
      "INFO:tensorflow:loss = 0.295095, step = 101 (3.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.7894\n",
      "INFO:tensorflow:loss = 0.163788, step = 201 (3.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8431\n",
      "INFO:tensorflow:loss = 0.141616, step = 301 (3.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8337\n",
      "INFO:tensorflow:loss = 0.120588, step = 401 (3.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8999\n",
      "INFO:tensorflow:loss = 0.161061, step = 501 (3.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9186\n",
      "INFO:tensorflow:loss = 0.190009, step = 601 (3.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.809\n",
      "INFO:tensorflow:loss = 0.138293, step = 701 (3.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9516\n",
      "INFO:tensorflow:loss = 0.210663, step = 801 (3.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8957\n",
      "INFO:tensorflow:loss = 0.159353, step = 901 (3.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8176\n",
      "INFO:tensorflow:loss = 0.19483, step = 1001 (3.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.791\n",
      "INFO:tensorflow:loss = 0.169472, step = 1101 (3.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9166\n",
      "INFO:tensorflow:loss = 0.224479, step = 1201 (3.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.7562\n",
      "INFO:tensorflow:loss = 0.170572, step = 1301 (3.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8882\n",
      "INFO:tensorflow:loss = 0.168142, step = 1401 (3.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9352\n",
      "INFO:tensorflow:loss = 0.0654277, step = 1501 (3.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.956\n",
      "INFO:tensorflow:loss = 0.206295, step = 1601 (3.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9162\n",
      "INFO:tensorflow:loss = 0.136688, step = 1701 (3.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8414\n",
      "INFO:tensorflow:loss = 0.0787666, step = 1801 (3.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.891\n",
      "INFO:tensorflow:loss = 0.209547, step = 1901 (3.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9216\n",
      "INFO:tensorflow:loss = 0.101378, step = 2001 (3.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8938\n",
      "INFO:tensorflow:loss = 0.180392, step = 2101 (3.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.7585\n",
      "INFO:tensorflow:loss = 0.173058, step = 2201 (3.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.831\n",
      "INFO:tensorflow:loss = 0.0934957, step = 2301 (3.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9211\n",
      "INFO:tensorflow:loss = 0.14113, step = 2401 (3.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.81\n",
      "INFO:tensorflow:loss = 0.1889, step = 2501 (3.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.7334\n",
      "INFO:tensorflow:loss = 0.111455, step = 2601 (3.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.754\n",
      "INFO:tensorflow:loss = 0.137888, step = 2701 (3.361 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.7607\n",
      "INFO:tensorflow:loss = 0.287427, step = 2801 (3.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8314\n",
      "INFO:tensorflow:loss = 0.0873343, step = 2901 (3.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8264\n",
      "INFO:tensorflow:loss = 0.12193, step = 3001 (3.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.6018\n",
      "INFO:tensorflow:loss = 0.131746, step = 3101 (3.378 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.717\n",
      "INFO:tensorflow:loss = 0.18486, step = 3201 (3.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.8912\n",
      "INFO:tensorflow:loss = 0.115826, step = 3301 (3.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 29.9101\n",
      "INFO:tensorflow:loss = 0.318079, step = 3401 (3.343 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3488 into /tmp/tmpy7f1dawf/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.18215.\n",
      "WARNING:tensorflow:From /home/tu/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'int64'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2017-10-07-08:48:31\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpy7f1dawf/model.ckpt-3488\n",
      "INFO:tensorflow:Finished evaluation at 2017-10-07-08:48:46\n",
      "INFO:tensorflow:Saving dict for global step 3488: accuracy = 0.963858, accuracy/baseline_label_mean = 0.0361417, accuracy/threshold_0.500000_mean = 0.963858, auc = 0.593819, auc_precision_recall = 0.0535692, global_step = 3488, labels/actual_label_mean = 0.0361417, labels/prediction_mean = 0.047488, loss = 0.155003, precision/positive_threshold_0.500000_mean = 0.0, recall/positive_threshold_0.500000_mean = 0.0\n",
      "AUC:  0.593819\n",
      "WARNING:tensorflow:From /home/tu/miniconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py:347: calling DNNClassifier.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_classes, or set `outputs` argument.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpy7f1dawf/model.ckpt-3488\n"
     ]
    }
   ],
   "source": [
    "col = [tf.feature_column.numeric_column('x', shape=x1.shape[1:])]\n",
    "clf = skflow.DNNClassifier(feature_columns=col, hidden_units=[600, 1200, 600], n_classes=2)\n",
    "clf.fit(input_fn=tf.estimator.inputs.numpy_input_fn(x={'x': x1}, y=y1, shuffle=False), steps=10000)\n",
    "auc = clf.evaluate(input_fn=tf.estimator.inputs.numpy_input_fn(x={'x': x2}, y=y2, num_epochs=1, shuffle=False))['auc']\n",
    "print('AUC: ', auc)\n",
    "\n",
    "preds = clf.predict(input_fn=tf.estimator.inputs.numpy_input_fn(x={'x': x2}, num_epochs=1, shuffle=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prep = list(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-bded48f2677c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"classes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-bded48f2677c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"classes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "predicted_classes = [p[\"classes\"] for p in prep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3da2fa9211d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"probabilities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Gini: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgini_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-80c10402e019>\u001b[0m in \u001b[0;36mgini_tf\u001b[0;34m(pred, y)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgini_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-80c10402e019>\u001b[0m in \u001b[0;36mgini\u001b[0;34m(actual, pred, cmpcol, sortcol)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgini\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmpcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msortcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtotalLosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds = [float(p[\"probabilities\"][1]) for p in preds]\n",
    "print('Gini: ', gini_tf(preds, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = clf.predict(input_fn=tf.estimator.inputs.numpy_input_fn(x={'x': xtest}, num_epochs=1, shuffle=False))\n",
    "preds = [float(p[\"probabilities\"][1]) for p in preds]\n",
    "test['target'] = preds\n",
    "test[['id','target']].to_csv('tf_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.0"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "trn_df = pd.read_csv(\"data/train.csv\")\n",
    "sub_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "id_test = sub_df.id\n",
    "target = trn_df[\"target\"]\n",
    "trn_df.drop(\"target\", axis = 1 , inplace = True)\n",
    "\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    trn_df[name1] = trn_df[f1].apply(lambda x: str(x)) + \"_\" + trn_df[f2].apply(lambda x: str(x))\n",
    "    sub_df[name1] = sub_df[f1].apply(lambda x: str(x)) + \"_\" + sub_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(trn_df[name1].values) + list(sub_df[name1].values))\n",
    "    trn_df[name1] = lbl.transform(list(trn_df[name1].values))\n",
    "    sub_df[name1] = lbl.transform(list(sub_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "trn_df = trn_df[train_features]\n",
    "sub_df = sub_df[train_features]\n",
    "\n",
    "f_cats = [f for f in trn_df.columns if \"_cat\" in f]\n",
    "\n",
    "for f in f_cats:\n",
    "    trn_df[f + \"_avg\"], sub_df[f + \"_avg\"] = target_encode(trn_series=trn_df[f],\n",
    "                                         tst_series=sub_df[f],\n",
    "                                         target=target,\n",
    "                                         min_samples_leaf=200,\n",
    "                                         smoothing=10,\n",
    "                                         noise_level=0)\n",
    "\n",
    "trn_df.ps_car_15 = trn_df.ps_car_15**2\n",
    "sub_df.ps_car_15 = sub_df.ps_car_15**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = trn_df.values\n",
    "y_all = keras.utils.np_utils.to_categorical(target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               6528      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 17,898\n",
      "Trainable params: 17,414\n",
      "Non-trainable params: 484\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "535690/535690 [==============================] - 19s 35us/step - loss: 0.2685 - acc: 0.9238\n",
      "Epoch 2/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1858 - acc: 0.9633\n",
      "Epoch 3/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1738 - acc: 0.9636\n",
      "Epoch 4/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1674 - acc: 0.9636\n",
      "Epoch 5/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1631 - acc: 0.9636\n",
      "Epoch 6/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1598 - acc: 0.9636\n",
      "Epoch 7/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1581 - acc: 0.9636\n",
      "Epoch 8/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1567 - acc: 0.9636\n",
      "Epoch 9/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1556 - acc: 0.9636\n",
      "Epoch 10/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1548 - acc: 0.9636\n",
      "Epoch 11/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1546 - acc: 0.9636\n",
      "Epoch 12/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1542 - acc: 0.9636\n",
      "Epoch 13/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1540 - acc: 0.9636\n",
      "Epoch 14/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1537 - acc: 0.9636\n",
      "Epoch 15/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1535 - acc: 0.9636\n",
      "Epoch 16/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1533 - acc: 0.9636\n",
      "Epoch 18/200\n",
      "535690/535690 [==============================] - 18s 33us/step - loss: 0.1532 - acc: 0.9636\n",
      "Epoch 19/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1531 - acc: 0.9636\n",
      "Epoch 20/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1531 - acc: 0.9636\n",
      "Epoch 21/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1528 - acc: 0.9636\n",
      "Epoch 22/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1528 - acc: 0.9636\n",
      "Epoch 23/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1528 - acc: 0.9636\n",
      "Epoch 24/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1527 - acc: 0.9636\n",
      "Epoch 25/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1525 - acc: 0.9636\n",
      "Epoch 26/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1526 - acc: 0.9636\n",
      "Epoch 27/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1526 - acc: 0.9636\n",
      "Epoch 28/200\n",
      "535690/535690 [==============================] - 18s 33us/step - loss: 0.1525 - acc: 0.9636\n",
      "Epoch 29/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1526 - acc: 0.9636\n",
      "Epoch 30/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1524 - acc: 0.9636\n",
      "Epoch 31/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1525 - acc: 0.9636\n",
      "Epoch 32/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1524 - acc: 0.9636\n",
      "Epoch 33/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1524 - acc: 0.9636\n",
      "Epoch 34/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1523 - acc: 0.9636\n",
      "Epoch 35/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1523 - acc: 0.9636\n",
      "Epoch 36/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1523 - acc: 0.9636\n",
      "Epoch 37/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1522 - acc: 0.9636\n",
      "Epoch 38/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1523 - acc: 0.9636\n",
      "Epoch 39/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1521 - acc: 0.9636\n",
      "Epoch 40/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1521 - acc: 0.9636\n",
      "Epoch 41/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1523 - acc: 0.9636\n",
      "Epoch 42/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1521 - acc: 0.9636\n",
      "Epoch 43/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1520 - acc: 0.9636\n",
      "Epoch 44/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1521 - acc: 0.9636\n",
      "Epoch 45/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1521 - acc: 0.9636\n",
      "Epoch 46/200\n",
      "535690/535690 [==============================] - 18s 33us/step - loss: 0.1520 - acc: 0.9636\n",
      "Epoch 47/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1521 - acc: 0.9636\n",
      "Epoch 48/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1520 - acc: 0.9636\n",
      "Epoch 49/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1520 - acc: 0.9636\n",
      "Epoch 50/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1519 - acc: 0.9636\n",
      "Epoch 51/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1519 - acc: 0.9636\n",
      "Epoch 52/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1519 - acc: 0.9636\n",
      "Epoch 53/200\n",
      "535690/535690 [==============================] - 18s 33us/step - loss: 0.1519 - acc: 0.9636\n",
      "Epoch 54/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1519 - acc: 0.9636\n",
      "Epoch 55/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1520 - acc: 0.9636\n",
      "Epoch 56/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1519 - acc: 0.9636\n",
      "Epoch 57/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1518 - acc: 0.9636\n",
      "Epoch 58/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1518 - acc: 0.9636\n",
      "Epoch 59/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1519 - acc: 0.9636\n",
      "Epoch 60/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1519 - acc: 0.9636\n",
      "Epoch 61/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1517 - acc: 0.9636\n",
      "Epoch 62/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1518 - acc: 0.9636\n",
      "Epoch 63/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1518 - acc: 0.9636\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1518 - acc: 0.9636\n",
      "Epoch 65/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1517 - acc: 0.9636\n",
      "Epoch 66/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1517 - acc: 0.9636\n",
      "Epoch 67/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1517 - acc: 0.9636\n",
      "Epoch 68/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1517 - acc: 0.9636\n",
      "Epoch 69/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1518 - acc: 0.9636\n",
      "Epoch 70/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1517 - acc: 0.9636\n",
      "Epoch 71/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1517 - acc: 0.9636\n",
      "Epoch 72/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1517 - acc: 0.9636\n",
      "Epoch 73/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1516 - acc: 0.9636\n",
      "Epoch 74/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: 0.1516 - acc: 0.9636\n",
      "Epoch 75/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 76/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 77/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 78/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 79/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 80/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 81/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 82/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 83/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 84/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 85/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 86/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 87/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 88/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 89/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 90/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 91/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 92/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 93/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 94/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 95/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 96/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 97/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 98/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 99/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 100/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 101/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 102/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 103/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 104/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 105/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 106/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 107/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 108/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 109/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 110/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 111/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 112/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 113/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 114/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 115/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 116/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 117/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 118/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 119/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 120/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 121/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 122/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 123/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 124/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 125/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 126/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 127/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 128/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 129/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 130/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 131/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 132/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 133/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 134/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 135/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 136/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 137/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 138/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 139/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 140/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 141/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 142/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 144/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 145/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 146/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 147/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 148/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 149/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 150/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 151/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 152/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 153/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 154/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 155/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 156/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 157/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 158/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 159/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 160/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 161/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 162/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 163/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 164/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 165/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 166/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 167/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 168/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 169/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 170/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 171/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 172/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 173/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 174/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 175/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 176/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 177/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 178/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 179/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 180/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 181/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 182/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 183/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 184/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 185/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 186/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 193/200\n",
      "535690/535690 [==============================] - 18s 33us/step - loss: nan - acc: 0.9636\n",
      "Epoch 194/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 195/200\n",
      "535690/535690 [==============================] - 18s 33us/step - loss: nan - acc: 0.9636\n",
      "Epoch 196/200\n",
      "535690/535690 [==============================] - 18s 33us/step - loss: nan - acc: 0.9636\n",
      "Epoch 197/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 198/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 199/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n",
      "Epoch 200/200\n",
      "535690/535690 [==============================] - 18s 34us/step - loss: nan - acc: 0.9636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f072000fe80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split train/valid datasets\n",
    "x_train, x_valid, y_train, y_valid = sklearn.model_selection.train_test_split(x_all, y_all, test_size=0.1, random_state=0)\n",
    "\n",
    "# Define model\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.normalization.BatchNormalization(input_shape=tuple([x_train.shape[1]]))) #Normalize Z before activation\n",
    "model.add(keras.layers.core.Dense(128, activation='relu'))  #apply activation function to Z after normalizing\n",
    "model.add(keras.layers.core.Dropout(rate=0.3)) # drop out 30% neurons of layer 1\n",
    "model.add(keras.layers.normalization.BatchNormalization())\n",
    "model.add(keras.layers.core.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.core.Dropout(rate=0.3)) # drop out 30% neurons of layer 2\n",
    "model.add(keras.layers.normalization.BatchNormalization())\n",
    "model.add(keras.layers.core.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.core.Dropout(rate=0.5)) # drop out 50% neurons of layer 3\n",
    "model.add(keras.layers.core.Dense(2, activation='sigmoid')) #sigmoid for binary classification\n",
    "\n",
    "# optimizer is Adadelta optimization algorithm\n",
    "# loss function is categorical_crossentropy\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\",metrics=[\"accuracy\"]) \n",
    "print(model.summary())\n",
    "\n",
    "# Use Early-Stopping\n",
    "#Early stopping is basically stopping the training once your loss starts to increase \n",
    "#(or in other words validation accuracy starts to decrease)\n",
    "#callback_early_stopping = keras.callbacks.EarlyStopping(monitor='loss', patience=10, verbose=0, mode='auto')\n",
    "\n",
    "# Train model\n",
    "model.fit(x_train, y_train, batch_size=1024, epochs=200, verbose=1) #,callbacks=[callback_early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test cross validation\n",
    "y_test = model.predict(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.94992745,  0.03313947],\n",
       "       [ 0.907556  ,  0.03448642],\n",
       "       [ 1.        ,  0.01953601],\n",
       "       ..., \n",
       "       [ 0.98985159,  0.03169642],\n",
       "       [ 0.98092467,  0.03235414],\n",
       "       [ 1.        ,  0.02161432]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25286866840384936"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gini_tf(y_test[:,1], y_valid[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
