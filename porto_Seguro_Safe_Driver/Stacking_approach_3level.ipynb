{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tu/miniconda3/envs/myenv/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import sklearn as lgbmsk\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(X_train, X_test, y_train, skf, clfs):\n",
    "    meta_train = np.zeros((X_train.shape[0], len(clfs)))\n",
    "    meta_test  = np.zeros((X_test.shape[0],  len(clfs)))\n",
    "    \n",
    "    for j, clf in enumerate(clfs):\n",
    "        print('Clf', j+1)\n",
    "        meta_test_j = np.zeros((X_test.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print('Fold', i+1)\n",
    "            X_tr = X_train[train]\n",
    "            y_tr = y_train[train]\n",
    "            X_ts = X_train[test]\n",
    "            y_ts = y_train[test]\n",
    "            clf.fit(X_tr, y_tr)\n",
    "            y_submission = clf.predict_proba(X_ts)[:, 1]\n",
    "            meta_train[test, j] = y_submission\n",
    "            meta_test_j[:, i] = clf.predict_proba(X_test)[:, 1]\n",
    "        meta_test[:, j] = meta_test_j.mean(1)\n",
    "        gc.collect()\n",
    "        \n",
    "    return meta_train, meta_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### one hot encoding + drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "y_train = train.target.values\n",
    "id_test = test['id'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#train['ps_ind_sum_bin'] = train[BIN].sum(axis = 1 )\n",
    "#train['ps_reg_mult'] = train.ps_reg_01 * train.ps_reg_02 * train.ps_reg_03\n",
    "#train['ps_car_13_ps_reg_03']= train.ps_car_13* train.ps_reg_03\n",
    "train['ps_car_15'] = (train.ps_car_15)**2\n",
    "train['ps_car_14'] = (train.ps_car_14)**2\n",
    "train['ps_car_12'] = round((train.ps_car_12)**2,4) * 10000\n",
    "train['ps_car_13'] = (train.ps_car_13)**2 * 48400\n",
    "train['ps_reg_03'] = (4*train.ps_reg_03)**2\n",
    "\n",
    "#test['ps_ind_sum_bin'] = test[BIN].sum(axis = 1 )\n",
    "#test['ps_reg_mult'] = test.ps_reg_01 * test.ps_reg_02 * test.ps_reg_03\n",
    "#test['ps_car_13_ps_reg_03']= test.ps_car_13* test.ps_reg_03\n",
    "test['ps_car_15'] = (test.ps_car_15)**2\n",
    "test['ps_car_14'] = (test.ps_car_14)**2\n",
    "test['ps_car_12'] = round((test.ps_car_12)**2,4) * 10000\n",
    "test['ps_car_13'] = (test.ps_car_13)**2 * 48400\n",
    "test['ps_reg_03'] = (4*test.ps_reg_03)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "skf = list(StratifiedKFold(y_train, n_folds=5, shuffle=True, random_state=115))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drop_columns = ['id', 'ps_calc_10', 'ps_calc_01', 'ps_calc_04', 'ps_car_02_cat', 'ps_calc_14',\n",
    "               'ps_calc_08', 'ps_calc_17_bin', 'ps_car_10_cat', 'ps_ind_11_bin', 'ps_calc_12',\n",
    "               'ps_calc_09', 'ps_car_06_cat', 'ps_calc_05','ps_calc_16_bin', 'ps_calc_20_bin',\n",
    "                'ps_calc_18_bin']\n",
    "\n",
    "train.drop(drop_columns, axis = 1, inplace = True)\n",
    "train.drop('target', axis = 1, inplace = True)\n",
    "test.drop(drop_columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CAT = []\n",
    "for col in train.columns:\n",
    "    if 'cat' in col:\n",
    "        CAT.append(col)\n",
    "        \n",
    "BIN = []\n",
    "for col in train.columns:\n",
    "    if 'bin' in col:\n",
    "        BIN.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "for col in CAT:\n",
    "    data = pd.concat((data, pd.get_dummies(data[col], prefix=col)), axis=1)\n",
    "    data.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = data.iloc[:train.shape[0],:]\n",
    "test = data.iloc[train.shape[0]:,:]\n",
    "test.index = range(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_n = np.array(train, dtype=np.float32)\n",
    "X_test_n = np.array(test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n"
     ]
    }
   ],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=800, criterion='gini', max_depth=37, max_features=25, \n",
    "                             min_samples_split=4, min_samples_leaf=2, n_jobs=-1, random_state=888),                  \n",
    "        XGBClassifier(n_estimators=600, learning_rate=0.03, max_depth=10, colsample_bytree=0.4, \n",
    "                      min_child_weight=1, seed=88888), \n",
    "        RandomForestClassifier(n_estimators=800, criterion='gini')]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_1 = pd.DataFrame(meta_train, index=X_train.index, columns=['base_et', 'base_xgb', 'base_rf'])\n",
    "meta_test_1 = pd.DataFrame(meta_test, index=X_test.index, columns=['base_et', 'base_xgb', 'base_rf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "meta_train_1 = pd.DataFrame(meta_train, index=train.index, columns=['base_et', 'base_xgb', 'base_rf'])\n",
    "meta_test_1 = pd.DataFrame(meta_test, index=test.index, columns=['base_et', 'base_xgb', 'base_rf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "meta_train_1.to_csv('save_meta/meta_train_1.csv', index = False)\n",
    "meta_test_1.to_csv('save_meta/meta_test_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Base Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "clfs = [LogisticRegression(C=1.0, penalty='l2', n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_2 = pd.DataFrame(meta_train, index=train.index, columns=['base_lr'])\n",
    "meta_test_2 = pd.DataFrame(meta_test, index=test.index, columns=['base_lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "meta_train_2.to_csv('save_meta/meta_train_2.csv', index = False)\n",
    "meta_test_2.to_csv('save_meta/meta_test_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### add target encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature                                 ps_reg_01_plus_ps_car_04_cat    2 in   0.0"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "\n",
    "def target_encode(trn_series=None,\n",
    "                  tst_series=None,\n",
    "                  target=None,\n",
    "                  min_samples_leaf=1,\n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior\n",
    "    \"\"\"\n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean\n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index\n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)\n",
    "\n",
    "gc.enable()\n",
    "\n",
    "trn_df = pd.read_csv(\"data/train.csv\")\n",
    "sub_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "id_test = sub_df.id\n",
    "target = trn_df[\"target\"]\n",
    "trn_df.drop(\"target\", axis = 1 , inplace = True)\n",
    "\n",
    "train_features = [\n",
    "    \"ps_car_13\",  #            : 1571.65 / shadow  609.23\n",
    "\t\"ps_reg_03\",  #            : 1408.42 / shadow  511.15\n",
    "\t\"ps_ind_05_cat\",  #        : 1387.87 / shadow   84.72\n",
    "\t\"ps_ind_03\",  #            : 1219.47 / shadow  230.55\n",
    "\t\"ps_ind_15\",  #            :  922.18 / shadow  242.00\n",
    "\t\"ps_reg_02\",  #            :  920.65 / shadow  267.50\n",
    "\t\"ps_car_14\",  #            :  798.48 / shadow  549.58\n",
    "\t\"ps_car_12\",  #            :  731.93 / shadow  293.62\n",
    "\t\"ps_car_01_cat\",  #        :  698.07 / shadow  178.72\n",
    "\t\"ps_car_07_cat\",  #        :  694.53 / shadow   36.35\n",
    "\t\"ps_ind_17_bin\",  #        :  620.77 / shadow   23.15\n",
    "\t\"ps_car_03_cat\",  #        :  611.73 / shadow   50.67\n",
    "\t\"ps_reg_01\",  #            :  598.60 / shadow  178.57\n",
    "\t\"ps_car_15\",  #            :  593.35 / shadow  226.43\n",
    "\t\"ps_ind_01\",  #            :  547.32 / shadow  154.58\n",
    "\t\"ps_ind_16_bin\",  #        :  475.37 / shadow   34.17\n",
    "\t\"ps_ind_07_bin\",  #        :  435.28 / shadow   28.92\n",
    "\t\"ps_car_06_cat\",  #        :  398.02 / shadow  212.43\n",
    "\t\"ps_car_04_cat\",  #        :  376.87 / shadow   76.98\n",
    "\t\"ps_ind_06_bin\",  #        :  370.97 / shadow   36.13\n",
    "\t\"ps_car_09_cat\",  #        :  214.12 / shadow   81.38\n",
    "\t\"ps_car_02_cat\",  #        :  203.03 / shadow   26.67\n",
    "\t\"ps_ind_02_cat\",  #        :  189.47 / shadow   65.68\n",
    "\t\"ps_car_11\",  #            :  173.28 / shadow   76.45\n",
    "\t\"ps_car_05_cat\",  #        :  172.75 / shadow   62.92\n",
    "\t\"ps_calc_09\",  #           :  169.13 / shadow  129.72\n",
    "\t\"ps_calc_05\",  #           :  148.83 / shadow  120.68\n",
    "\t\"ps_ind_08_bin\",  #        :  140.73 / shadow   27.63\n",
    "\t\"ps_car_08_cat\",  #        :  120.87 / shadow   28.82\n",
    "\t\"ps_ind_09_bin\",  #        :  113.92 / shadow   27.05\n",
    "\t\"ps_ind_04_cat\",  #        :  107.27 / shadow   37.43\n",
    "\t\"ps_ind_18_bin\",  #        :   77.42 / shadow   25.97\n",
    "\t\"ps_ind_12_bin\",  #        :   39.67 / shadow   15.52\n",
    "\t\"ps_ind_14\",  #            :   37.37 / shadow   16.65\n",
    "]\n",
    "# add combinations\n",
    "combs = [\n",
    "    ('ps_reg_01', 'ps_car_02_cat'),  \n",
    "    ('ps_reg_01', 'ps_car_04_cat'),\n",
    "]\n",
    "start = time.time()\n",
    "for n_c, (f1, f2) in enumerate(combs):\n",
    "    name1 = f1 + \"_plus_\" + f2\n",
    "    print('current feature %60s %4d in %5.1f'\n",
    "          % (name1, n_c + 1, (time.time() - start) / 60), end='')\n",
    "    print('\\r' * 75, end='')\n",
    "    trn_df[name1] = trn_df[f1].apply(lambda x: str(x)) + \"_\" + trn_df[f2].apply(lambda x: str(x))\n",
    "    sub_df[name1] = sub_df[f1].apply(lambda x: str(x)) + \"_\" + sub_df[f2].apply(lambda x: str(x))\n",
    "    # Label Encode\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(trn_df[name1].values) + list(sub_df[name1].values))\n",
    "    trn_df[name1] = lbl.transform(list(trn_df[name1].values))\n",
    "    sub_df[name1] = lbl.transform(list(sub_df[name1].values))\n",
    "\n",
    "    train_features.append(name1)\n",
    "    \n",
    "trn_df = trn_df[train_features]\n",
    "sub_df = sub_df[train_features]\n",
    "\n",
    "f_cats = [f for f in trn_df.columns if \"_cat\" in f]\n",
    "\n",
    "for f in f_cats:\n",
    "    trn_df[f + \"_avg\"], sub_df[f + \"_avg\"] = target_encode(trn_series=trn_df[f],\n",
    "                                         tst_series=sub_df[f],\n",
    "                                         target=target,\n",
    "                                         min_samples_leaf=200,\n",
    "                                         smoothing=10,\n",
    "                                         noise_level=0)\n",
    "\n",
    "trn_df.ps_car_15 = trn_df.ps_car_15**2\n",
    "sub_df.ps_car_15 = sub_df.ps_car_15**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_n = np.array(trn_df, dtype=np.float32)\n",
    "X_test_n = np.array(sub_df, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "        \n",
    "# LightGBM params\n",
    "lgb_params_1 = {\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 1250,\n",
    "    'max_bin': 10,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 10,\n",
    "    'colsample_bytree': 0.8, \n",
    "    'min_child_samples': 500\n",
    "}\n",
    "\n",
    "lgb_params_2 = {\n",
    "    'learning_rate': 0.005,\n",
    "    'n_estimators': 3700,\n",
    "    'subsample': 0.7,\n",
    "    'subsample_freq': 2,\n",
    "    'colsample_bytree': 0.3,  \n",
    "    'num_leaves': 16\n",
    "}\n",
    "\n",
    "lgb_params_3 = {\n",
    "    'learning_rate': 0.02,\n",
    "    'n_estimators': 800,\n",
    "    'max_depth': 4\n",
    "}\n",
    "\n",
    "\n",
    "# RandomForest params\n",
    "rf_params = {}\n",
    "rf_params['n_estimators'] = 200\n",
    "rf_params['max_depth'] = 6\n",
    "rf_params['min_samples_split'] = 70\n",
    "rf_params['min_samples_leaf'] = 30\n",
    "\n",
    "\n",
    "# ExtraTrees params\n",
    "et_params = {}\n",
    "et_params['n_estimators'] = 155\n",
    "et_params['max_features'] = 0.3\n",
    "et_params['max_depth'] = 6\n",
    "et_params['min_samples_split'] = 40\n",
    "et_params['min_samples_leaf'] = 18\n",
    "\n",
    "\n",
    "# XGBoost params\n",
    "xgb_params = {\n",
    "        'objective' : 'binary:logistic',\n",
    "        'learning_rate' : 0.02,\n",
    "        'n_estimators' : 1000,\n",
    "        'gamma' : 9,\n",
    "        'max_depth' : 4,\n",
    "        'subsample' : 0.9,\n",
    "        'colsample_bytree' : 0.9,  \n",
    "        'min_child_weight' : 10\n",
    "}\n",
    "\n",
    "# CatBoost params\n",
    "cat_params = {\n",
    "    'iterations' : 900,\n",
    "        'depth' : 8,\n",
    "        'rsm' : 0.95,\n",
    "        'learning_rate' : 0.03,\n",
    "        'l2_leaf_reg' : 3.5 , \n",
    "        'border_count' : 8,\n",
    "        'gradient_iterations' : 4\n",
    "}\n",
    "lgb_model_1 = LGBMClassifier(**lgb_params_1)\n",
    "\n",
    "lgb_model_2 = LGBMClassifier(**lgb_params_2)\n",
    "\n",
    "lgb_model_3 = LGBMClassifier(**lgb_params_3)\n",
    "\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "\n",
    "et_model = ExtraTreesClassifier(**et_params)\n",
    "        \n",
    "xgb_model = XGBClassifier(**xgb_params)\n",
    "\n",
    "cat_model = CatBoostClassifier(**cat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clfs = [lgb_model_1,\n",
    "        lgb_model_2,\n",
    "        lgb_model_3,\n",
    "        xgb_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 2\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 3\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 4\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n"
     ]
    }
   ],
   "source": [
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_3 = pd.DataFrame(meta_train, index=train.index, columns=['base_lgb_1', 'base_lgb_2', 'base_lgb_3','base_xgb_2'])\n",
    "meta_test_3 = pd.DataFrame(meta_test, index=test.index, columns=['base_lgb_1', 'base_lgb_2', 'base_lgb_3','base_xgb_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "meta_train_3.to_csv('save_meta/meta_train_3.csv', index = False)\n",
    "meta_test_3.to_csv('save_meta/meta_test_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_ind_16_bin ps_ind_17_bin\n",
      "ps_ind_16_bin ps_ind_18_bin\n",
      "ps_ind_17_bin ps_ind_16_bin\n",
      "ps_ind_17_bin ps_ind_18_bin\n",
      "ps_ind_18_bin ps_ind_16_bin\n",
      "ps_ind_18_bin ps_ind_17_bin\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "y_train = train.target.values\n",
    "id_test = test['id'].values\n",
    "\n",
    "\n",
    "train['ps_ind_0609_bin'] = train.apply(lambda x: 1 if x['ps_ind_06_bin'] == 1 else (2 if x['ps_ind_07_bin'] == 1 else \n",
    "    (\n",
    "        3 if x['ps_ind_08_bin'] == 1 else (4 if x['ps_ind_09_bin'] == 1 else 5)\n",
    "\n",
    ")), axis = 1)\n",
    "\n",
    "test['ps_ind_0609_bin'] = test.apply(lambda x: 1 if x['ps_ind_06_bin'] == 1 else (2 if x['ps_ind_07_bin'] == 1 else \n",
    "(\n",
    "3 if x['ps_ind_08_bin'] == 1 else (4 if x['ps_ind_09_bin'] == 1 else 5)\n",
    "\n",
    ")), axis = 1)\n",
    "\n",
    "train.drop(['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin'], axis = 1, inplace = True)\n",
    "\n",
    "test.drop(['ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin', 'ps_ind_09_bin'], axis = 1, inplace = True)\n",
    "\n",
    "train['ps_car_13'] = (train['ps_car_13']*train['ps_car_13']* 48400).round(0)\n",
    "\n",
    "test['ps_car_13'] = (test['ps_car_13']*test['ps_car_13']* 48400).round(0)\n",
    "\n",
    "train['ps_car_12'] = (train['ps_car_12']*train['ps_car_12']).round(4) * 10000\n",
    "\n",
    "test['ps_car_12'] = (test['ps_car_12']*test['ps_car_12']).round(4) * 10000\n",
    "\n",
    "for c in train[[c for c in train.columns if 'bin' in c]].columns:\n",
    "    for cc in train[[c for c in train.columns if 'bin' in c]].columns:\n",
    "            if train[train[cc] * train[c] == 0].shape[0] == train.shape[0]:\n",
    "                print(c, cc)\n",
    "\n",
    "train['ps_ind_161718_bin'] = train.apply(lambda x: 1 if x['ps_ind_16_bin'] == 1 else\n",
    "                                        (2 if x['ps_ind_17_bin'] == 1 else 3), axis = 1\n",
    "                                        )\n",
    "\n",
    "test['ps_ind_161718_bin'] = test.apply(lambda x: 1 if x['ps_ind_16_bin'] == 1 else\n",
    "                                        (2 if x['ps_ind_17_bin'] == 1 else 3), axis = 1\n",
    "                                        )\n",
    "\n",
    "train.drop(['ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin'], axis = 1, inplace = True)\n",
    "\n",
    "test.drop(['ps_ind_16_bin', 'ps_ind_17_bin', 'ps_ind_18_bin'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 34) (892816, 33)\n"
     ]
    }
   ],
   "source": [
    "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "train = train.drop(col_to_drop, axis=1)  \n",
    "test = test.drop(col_to_drop, axis=1)  \n",
    "\n",
    "for c in train.select_dtypes(include=['float64']).columns:\n",
    "    train[c]=train[c].astype(np.float32)\n",
    "    test[c]=test[c].astype(np.float32)\n",
    "for c in train.select_dtypes(include=['int64']).columns[2:]:\n",
    "    train[c]=train[c].astype(np.int8)\n",
    "    test[c]=test[c].astype(np.int8)    \n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "# custom objective function (similar to auc)\n",
    "\n",
    "def gini(y, pred):\n",
    "    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(y) + 1) / 2.\n",
    "    return gs / len(y)\n",
    "\n",
    "def gini_xgb(pred, y):\n",
    "    y = y.get_label()\n",
    "    return 'gini', gini(y, pred) / gini(y, y)\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    y = list(dtrain.get_label())\n",
    "    score = gini(y, preds) / gini(y, y)\n",
    "    return 'gini', score, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " xgb kfold: 1  of  5 : \n",
      "[0]\ttrain-auc:0.597993\tvalid-auc:0.577773\ttrain-gini:0.194886\tvalid-gini:0.157185\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-auc:0.624611\tvalid-auc:0.601221\ttrain-gini:0.249222\tvalid-gini:0.202427\n",
      "[200]\ttrain-auc:0.637785\tvalid-auc:0.615977\ttrain-gini:0.27557\tvalid-gini:0.231954\n",
      "[300]\ttrain-auc:0.646992\tvalid-auc:0.625348\ttrain-gini:0.293984\tvalid-gini:0.250696\n",
      "[400]\ttrain-auc:0.65341\tvalid-auc:0.629984\ttrain-gini:0.30682\tvalid-gini:0.259967\n",
      "[500]\ttrain-auc:0.658329\tvalid-auc:0.632275\ttrain-gini:0.316658\tvalid-gini:0.264549\n",
      "[600]\ttrain-auc:0.66221\tvalid-auc:0.633597\ttrain-gini:0.324421\tvalid-gini:0.267194\n",
      "[700]\ttrain-auc:0.665484\tvalid-auc:0.634453\ttrain-gini:0.330969\tvalid-gini:0.268907\n",
      "[800]\ttrain-auc:0.668549\tvalid-auc:0.635254\ttrain-gini:0.337099\tvalid-gini:0.270508\n",
      "[900]\ttrain-auc:0.671388\tvalid-auc:0.635552\ttrain-gini:0.342776\tvalid-gini:0.271103\n",
      "[1000]\ttrain-auc:0.674172\tvalid-auc:0.635737\ttrain-gini:0.348343\tvalid-gini:0.271475\n",
      "[1100]\ttrain-auc:0.676827\tvalid-auc:0.635609\ttrain-gini:0.353655\tvalid-gini:0.271217\n",
      "Stopping. Best iteration:\n",
      "[1005]\ttrain-auc:0.674318\tvalid-auc:0.635814\ttrain-gini:0.348635\tvalid-gini:0.271628\n",
      "\n",
      " xgb kfold: 2  of  5 : \n",
      "[0]\ttrain-auc:0.59403\tvalid-auc:0.598267\ttrain-gini:0.189452\tvalid-gini:0.193314\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-auc:0.622706\tvalid-auc:0.621891\ttrain-gini:0.245421\tvalid-gini:0.243788\n",
      "[200]\ttrain-auc:0.636209\tvalid-auc:0.635263\ttrain-gini:0.272419\tvalid-gini:0.270527\n",
      "[300]\ttrain-auc:0.645842\tvalid-auc:0.641798\ttrain-gini:0.291684\tvalid-gini:0.283595\n",
      "[400]\ttrain-auc:0.652232\tvalid-auc:0.644577\ttrain-gini:0.304464\tvalid-gini:0.289154\n",
      "[500]\ttrain-auc:0.656969\tvalid-auc:0.646147\ttrain-gini:0.313939\tvalid-gini:0.292294\n",
      "[600]\ttrain-auc:0.660699\tvalid-auc:0.647057\ttrain-gini:0.321397\tvalid-gini:0.294115\n",
      "[700]\ttrain-auc:0.663989\tvalid-auc:0.647496\ttrain-gini:0.327978\tvalid-gini:0.294991\n",
      "[800]\ttrain-auc:0.667331\tvalid-auc:0.647737\ttrain-gini:0.334663\tvalid-gini:0.295473\n",
      "[900]\ttrain-auc:0.670233\tvalid-auc:0.647901\ttrain-gini:0.340466\tvalid-gini:0.295802\n",
      "Stopping. Best iteration:\n",
      "[876]\ttrain-auc:0.66953\tvalid-auc:0.647919\ttrain-gini:0.339061\tvalid-gini:0.295838\n",
      "\n",
      " xgb kfold: 3  of  5 : \n",
      "[0]\ttrain-auc:0.597773\tvalid-auc:0.592625\ttrain-gini:0.193339\tvalid-gini:0.181806\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-auc:0.623856\tvalid-auc:0.615046\ttrain-gini:0.24771\tvalid-gini:0.230099\n",
      "[200]\ttrain-auc:0.636658\tvalid-auc:0.62594\ttrain-gini:0.273316\tvalid-gini:0.251879\n",
      "[300]\ttrain-auc:0.646841\tvalid-auc:0.633936\ttrain-gini:0.293682\tvalid-gini:0.267872\n",
      "[400]\ttrain-auc:0.653014\tvalid-auc:0.637635\ttrain-gini:0.306027\tvalid-gini:0.27527\n",
      "[500]\ttrain-auc:0.657554\tvalid-auc:0.63906\ttrain-gini:0.315107\tvalid-gini:0.27812\n",
      "[600]\ttrain-auc:0.661453\tvalid-auc:0.640283\ttrain-gini:0.322906\tvalid-gini:0.280566\n",
      "[700]\ttrain-auc:0.664731\tvalid-auc:0.64068\ttrain-gini:0.329462\tvalid-gini:0.28136\n",
      "[800]\ttrain-auc:0.667944\tvalid-auc:0.641365\ttrain-gini:0.335888\tvalid-gini:0.28273\n",
      "[900]\ttrain-auc:0.670888\tvalid-auc:0.641525\ttrain-gini:0.341776\tvalid-gini:0.283049\n",
      "[1000]\ttrain-auc:0.673435\tvalid-auc:0.641792\ttrain-gini:0.346871\tvalid-gini:0.283583\n",
      "[1100]\ttrain-auc:0.676094\tvalid-auc:0.64176\ttrain-gini:0.352188\tvalid-gini:0.28352\n",
      "Stopping. Best iteration:\n",
      "[1004]\ttrain-auc:0.673555\tvalid-auc:0.641814\ttrain-gini:0.347111\tvalid-gini:0.283627\n",
      "\n",
      " xgb kfold: 4  of  5 : \n",
      "[0]\ttrain-auc:0.596068\tvalid-auc:0.59846\ttrain-gini:0.192999\tvalid-gini:0.196214\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-auc:0.623133\tvalid-auc:0.621803\ttrain-gini:0.24627\tvalid-gini:0.243599\n",
      "[200]\ttrain-auc:0.636755\tvalid-auc:0.632181\ttrain-gini:0.273511\tvalid-gini:0.264362\n",
      "[300]\ttrain-auc:0.645862\tvalid-auc:0.637833\ttrain-gini:0.291723\tvalid-gini:0.275667\n",
      "[400]\ttrain-auc:0.652384\tvalid-auc:0.64167\ttrain-gini:0.304768\tvalid-gini:0.283339\n",
      "[500]\ttrain-auc:0.657122\tvalid-auc:0.64404\ttrain-gini:0.314244\tvalid-gini:0.28808\n",
      "[600]\ttrain-auc:0.660907\tvalid-auc:0.645344\ttrain-gini:0.321813\tvalid-gini:0.290688\n",
      "[700]\ttrain-auc:0.664463\tvalid-auc:0.645953\ttrain-gini:0.328925\tvalid-gini:0.291906\n",
      "[800]\ttrain-auc:0.667507\tvalid-auc:0.646228\ttrain-gini:0.335013\tvalid-gini:0.292457\n",
      "[900]\ttrain-auc:0.670306\tvalid-auc:0.646549\ttrain-gini:0.340613\tvalid-gini:0.293097\n",
      "Stopping. Best iteration:\n",
      "[878]\ttrain-auc:0.669712\tvalid-auc:0.64662\ttrain-gini:0.339423\tvalid-gini:0.29324\n",
      "\n",
      " xgb kfold: 5  of  5 : \n",
      "[0]\ttrain-auc:0.596197\tvalid-auc:0.595501\ttrain-gini:0.192577\tvalid-gini:0.195418\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-auc:0.623308\tvalid-auc:0.61905\ttrain-gini:0.246619\tvalid-gini:0.238117\n",
      "[200]\ttrain-auc:0.638011\tvalid-auc:0.627886\ttrain-gini:0.276023\tvalid-gini:0.255772\n",
      "[300]\ttrain-auc:0.64749\tvalid-auc:0.633139\ttrain-gini:0.294981\tvalid-gini:0.266277\n",
      "[400]\ttrain-auc:0.653941\tvalid-auc:0.635942\ttrain-gini:0.307882\tvalid-gini:0.271884\n",
      "[500]\ttrain-auc:0.658506\tvalid-auc:0.63703\ttrain-gini:0.317012\tvalid-gini:0.274061\n",
      "[600]\ttrain-auc:0.662269\tvalid-auc:0.636863\ttrain-gini:0.324537\tvalid-gini:0.273726\n",
      "Stopping. Best iteration:\n",
      "[511]\ttrain-auc:0.658994\tvalid-auc:0.637103\ttrain-gini:0.317988\tvalid-gini:0.274206\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sub_lgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-19231023ec36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# lgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0msub_lgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0msub_train_lgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sub_lgb' is not defined"
     ]
    }
   ],
   "source": [
    "# xgb\n",
    "params = {'eta': 0.02, 'max_depth': 4, 'subsample': 0.9, 'colsample_bytree': 0.9, \n",
    "          'objective': 'binary:logistic', 'eval_metric': 'auc', 'silent': True}\n",
    "\n",
    "X = train.drop(['id', 'target'], axis=1)\n",
    "features = X.columns\n",
    "X = X.values\n",
    "y = train['target'].values\n",
    "sub=test['id'].to_frame()\n",
    "sub['target']=0\n",
    "\n",
    "sub_train = train['id'].to_frame()\n",
    "sub_train['target']=0\n",
    "\n",
    "nrounds=10**6  # need to change to 2000\n",
    "kfold = 5  # need to change to 5\n",
    "skf = StratifiedShuffleSplit( n_splits=kfold, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(' xgb kfold: {}  of  {} : '.format(i+1, kfold))\n",
    "    X_train, X_valid = X[train_index], X[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    d_train = xgb.DMatrix(X_train, y_train) \n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid) \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "    xgb_model = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n",
    "                          feval=gini_xgb, maximize=True, verbose_eval=100)\n",
    "    sub['target'] += xgb_model.predict(xgb.DMatrix(test[features].values), \n",
    "                        ntree_limit=xgb_model.best_ntree_limit+50) / (kfold)\n",
    "    \n",
    "    sub_train['target'] += xgb_model.predict(xgb.DMatrix(train[features].values), \n",
    "                        ntree_limit=xgb_model.best_ntree_limit+50) / (kfold)\n",
    "    \n",
    "gc.collect()\n",
    "sub.head(2)\n",
    "\n",
    "#sub.to_csv('save_meta/test_sub_xgb.csv', index=False, float_format='%.5f')\n",
    "#sub_train.to_csv('save_meta/train_sub_xgb.csv', index=False, float_format='%.5f')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lgb kfold: 1  of  5 : \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.621403\tvalid_0's gini: 0.242806\n",
      "[200]\tvalid_0's auc: 0.622771\tvalid_0's gini: 0.245543\n",
      "[300]\tvalid_0's auc: 0.625825\tvalid_0's gini: 0.25165\n",
      "[400]\tvalid_0's auc: 0.628044\tvalid_0's gini: 0.256088\n",
      "[500]\tvalid_0's auc: 0.630923\tvalid_0's gini: 0.261846\n",
      "[600]\tvalid_0's auc: 0.633248\tvalid_0's gini: 0.266496\n",
      "[700]\tvalid_0's auc: 0.634877\tvalid_0's gini: 0.269754\n",
      "[800]\tvalid_0's auc: 0.635977\tvalid_0's gini: 0.271955\n",
      "[900]\tvalid_0's auc: 0.636836\tvalid_0's gini: 0.273672\n",
      "[1000]\tvalid_0's auc: 0.637252\tvalid_0's gini: 0.274504\n",
      "[1100]\tvalid_0's auc: 0.637248\tvalid_0's gini: 0.274496\n",
      "Early stopping, best iteration is:\n",
      "[1003]\tvalid_0's auc: 0.637299\tvalid_0's gini: 0.274598\n",
      " lgb kfold: 2  of  5 : \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.611369\tvalid_0's gini: 0.222739\n",
      "[200]\tvalid_0's auc: 0.614874\tvalid_0's gini: 0.229747\n",
      "[300]\tvalid_0's auc: 0.618546\tvalid_0's gini: 0.237092\n",
      "[400]\tvalid_0's auc: 0.621793\tvalid_0's gini: 0.243586\n",
      "[500]\tvalid_0's auc: 0.624732\tvalid_0's gini: 0.249464\n",
      "[600]\tvalid_0's auc: 0.627449\tvalid_0's gini: 0.254899\n",
      "[700]\tvalid_0's auc: 0.629153\tvalid_0's gini: 0.258307\n",
      "[800]\tvalid_0's auc: 0.629875\tvalid_0's gini: 0.25975\n",
      "[900]\tvalid_0's auc: 0.629853\tvalid_0's gini: 0.259706\n",
      "Early stopping, best iteration is:\n",
      "[827]\tvalid_0's auc: 0.630086\tvalid_0's gini: 0.260172\n",
      " lgb kfold: 3  of  5 : \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.619585\tvalid_0's gini: 0.239171\n",
      "[200]\tvalid_0's auc: 0.621094\tvalid_0's gini: 0.242189\n",
      "[300]\tvalid_0's auc: 0.623786\tvalid_0's gini: 0.247572\n",
      "[400]\tvalid_0's auc: 0.627746\tvalid_0's gini: 0.255491\n",
      "[500]\tvalid_0's auc: 0.631638\tvalid_0's gini: 0.263277\n",
      "[600]\tvalid_0's auc: 0.634435\tvalid_0's gini: 0.26887\n",
      "[700]\tvalid_0's auc: 0.636324\tvalid_0's gini: 0.272647\n",
      "[800]\tvalid_0's auc: 0.637019\tvalid_0's gini: 0.274038\n",
      "[900]\tvalid_0's auc: 0.637722\tvalid_0's gini: 0.275444\n",
      "[1000]\tvalid_0's auc: 0.637771\tvalid_0's gini: 0.275541\n",
      "[1100]\tvalid_0's auc: 0.638064\tvalid_0's gini: 0.276128\n",
      "Early stopping, best iteration is:\n",
      "[1099]\tvalid_0's auc: 0.638066\tvalid_0's gini: 0.276132\n",
      " lgb kfold: 4  of  5 : \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.631766\tvalid_0's gini: 0.263532\n",
      "[200]\tvalid_0's auc: 0.634517\tvalid_0's gini: 0.269033\n",
      "[300]\tvalid_0's auc: 0.637652\tvalid_0's gini: 0.275304\n",
      "[400]\tvalid_0's auc: 0.640991\tvalid_0's gini: 0.281981\n",
      "[500]\tvalid_0's auc: 0.643395\tvalid_0's gini: 0.286791\n",
      "[600]\tvalid_0's auc: 0.645223\tvalid_0's gini: 0.290446\n",
      "[700]\tvalid_0's auc: 0.646944\tvalid_0's gini: 0.293888\n",
      "[800]\tvalid_0's auc: 0.648168\tvalid_0's gini: 0.296336\n",
      "[900]\tvalid_0's auc: 0.648575\tvalid_0's gini: 0.297149\n",
      "[1000]\tvalid_0's auc: 0.64876\tvalid_0's gini: 0.29752\n",
      "[1100]\tvalid_0's auc: 0.648988\tvalid_0's gini: 0.297975\n",
      "[1200]\tvalid_0's auc: 0.648879\tvalid_0's gini: 0.297758\n",
      "Early stopping, best iteration is:\n",
      "[1101]\tvalid_0's auc: 0.648995\tvalid_0's gini: 0.29799\n",
      " lgb kfold: 5  of  5 : \n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's auc: 0.630922\tvalid_0's gini: 0.26184\n",
      "[200]\tvalid_0's auc: 0.63294\tvalid_0's gini: 0.265881\n",
      "[300]\tvalid_0's auc: 0.635651\tvalid_0's gini: 0.271302\n",
      "[400]\tvalid_0's auc: 0.638893\tvalid_0's gini: 0.277786\n",
      "[500]\tvalid_0's auc: 0.642079\tvalid_0's gini: 0.284158\n",
      "[600]\tvalid_0's auc: 0.644801\tvalid_0's gini: 0.289603\n",
      "[700]\tvalid_0's auc: 0.646602\tvalid_0's gini: 0.293203\n",
      "[800]\tvalid_0's auc: 0.647451\tvalid_0's gini: 0.294903\n",
      "[900]\tvalid_0's auc: 0.648172\tvalid_0's gini: 0.296344\n",
      "[1000]\tvalid_0's auc: 0.648607\tvalid_0's gini: 0.297214\n",
      "[1100]\tvalid_0's auc: 0.648931\tvalid_0's gini: 0.297861\n",
      "[1200]\tvalid_0's auc: 0.649026\tvalid_0's gini: 0.298051\n",
      "Early stopping, best iteration is:\n",
      "[1185]\tvalid_0's auc: 0.649166\tvalid_0's gini: 0.298332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1871"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lgb\n",
    "sub_lgb =test['id'].to_frame()\n",
    "sub_train_lgb = train['id'].to_frame()\n",
    "\n",
    "sub_lgb['target']=0\n",
    "sub_train_lgb['target']=0\n",
    "\n",
    "params = {'metric': 'auc', 'learning_rate' : 0.01, 'max_depth':8, 'max_bin':10,  'objective': 'binary', \n",
    "          'feature_fraction': 0.8,'bagging_fraction':0.9,'bagging_freq':5,  'min_data': 500}\n",
    "\n",
    "skf = StratifiedShuffleSplit( n_splits=kfold, random_state=1)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(' lgb kfold: {}  of  {} : '.format(i+1, kfold))\n",
    "    X_train, X_eval = X[train_index], X[test_index]\n",
    "    y_train, y_eval = y[train_index], y[test_index]\n",
    "    lgb_model = lgb.train(params, lgb.Dataset(X_train, label=y_train), nrounds, \n",
    "                  lgb.Dataset(X_eval, label=y_eval), verbose_eval=100, \n",
    "                  feval=gini_lgb, early_stopping_rounds=100)\n",
    "    sub_lgb['target'] += lgb_model.predict(test[features].values, \n",
    "                        num_iteration=lgb_model.best_iteration) / (kfold)\n",
    "    sub_train_lgb['target'] += lgb_model.predict(train[features].values, \n",
    "                        num_iteration=lgb_model.best_iteration) / (kfold)\n",
    "    \n",
    "#sub.to_csv('save_meta/test_sub_lgb.csv', index=False, float_format='%.5f') \n",
    "#sub_train.to_csv('save_meta/train_sub_lgb.csv', index=False, float_format='%.5f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5403"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_train_4 = pd.concat((sub_train, sub_train_lgb), axis =1).drop('id', axis = 1 )\n",
    "meta_test_4 = pd.concat((sub, sub_lgb), axis = 1).drop('id', axis = 1 )\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_4.columns= ['xgb2','lgb2']\n",
    "meta_test_4.columns = ['xgb2','lgb2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_4.to_csv('save_meta/meta_train_4.csv', index = False)\n",
    "meta_test_4.to_csv('save_meta/meta_test_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Stacking level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "y_train = train['target'].values\n",
    "id_test = test['id'].values\n",
    "\n",
    "train.drop(['id', 'target'], axis=1, inplace=True)\n",
    "test.drop(['id'], axis=1, inplace=True)\n",
    "y_train = pd.read_csv('data/train.csv')['target'].values\n",
    "\n",
    "CAT = []\n",
    "for col in train.columns:\n",
    "    if 'cat' in col:\n",
    "        CAT.append(col)\n",
    "        \n",
    "BIN = []\n",
    "for col in train.columns:\n",
    "    if 'bin' in col:\n",
    "        BIN.append(col)\n",
    "\n",
    "X_train, X_test = preprocess_data(train, test, BIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat((X_train, X_test), axis=0, ignore_index=True)\n",
    "for col in CAT:\n",
    "    data = pd.concat((data, pd.get_dummies(data[col], prefix=col)), axis=1)\n",
    "    data.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tu/miniconda3/envs/env36/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_train = data.ix[:(X_train.shape[0]-1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test = data.ix[X_train.shape[0]:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train['ps_car_15_carre'] = (X_train.ps_car_15)**2\n",
    "X_test['ps_car_15_carre'] = (X_test.ps_car_15)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(['ps_ind_10_bin','ps_ind_11_bin','ps_ind_12_bin',\\\n",
    "                'ps_ind_13_bin','ps_car_15'], axis=1)\n",
    "X_test = X_test.drop(['ps_ind_10_bin','ps_ind_11_bin','ps_ind_12_bin',\\\n",
    "                'ps_ind_13_bin','ps_car_15'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_2 = pd.concat([X_train, meta_train_level1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test_2 = pd.concat([X_test, meta_test_level1], axis = 1, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_n = np.array(X_train_2, dtype=np.float32)\n",
    "X_test_n = np.array(X_test_2, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "skf = list(StratifiedKFold(y_train, n_folds=5, shuffle=True, random_state=115))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clf 1\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Clf 2\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n"
     ]
    }
   ],
   "source": [
    "clfs = [ExtraTreesClassifier(n_estimators=1000, max_features=25, criterion='gini', min_samples_split=2, \n",
    "                             max_depth=36, min_samples_leaf=2, n_jobs=-1, random_state=888),\n",
    "        XGBClassifier(n_estimators=600, learning_rate=0.03, max_depth=10, colsample_bytree=0.4, \n",
    "                      min_child_weight=1, seed=129),\n",
    "        CatBoostClassifier(iterations= 800, learning_rate=0.01),\n",
    "        LGBMClassifier(max_depth=10, learning_rate=0.03, n_estimators=500, subsample_for_bin=500000, \n",
    "                       min_child_weight=5, min_child_samples=10, subsample=1.0, subsample_freq=1, \n",
    "                       colsample_bytree=1.0, random_state= 130, n_jobs=-1)]\n",
    "\n",
    "meta_train, meta_test = stacking(X_train_n, X_test_n, y_train, skf, clfs)\n",
    "\n",
    "meta_train_0 = pd.DataFrame(meta_train, index=X_train.index, columns=['main_et', 'main_xgb','main_catboost','main_lgb'])\n",
    "meta_test_0 = pd.DataFrame(meta_test, index=X_test.index, columns=['main_et', 'main_xgb','main_catboost','main_lgb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "meta_train_0.to_csv('save_meta/meta_train_0.csv', index = False)\n",
    "meta_test_0.to_csv('save_meta/meta_test_0.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "meta_train_0 = pd.read_csv('save_meta/meta_train_0.csv')\n",
    "meta_test_0 = pd.read_csv('save_meta/meta_test_0.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "y_train = train['target'].values\n",
    "id_test = test['id'].values\n",
    "\n",
    "train.drop(['id', 'target'], axis=1, inplace=True)\n",
    "test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "CAT = []\n",
    "for col in train.columns:\n",
    "    if 'cat' in col:\n",
    "        CAT.append(col)\n",
    "        \n",
    "BIN = []\n",
    "for col in train.columns:\n",
    "    if 'bin' in col:\n",
    "        BIN.append(col)\n",
    "\n",
    "#X_train, X_test = preprocess_data(train, test, BIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train\n",
    "X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon(reg):\n",
    "    integer = int(np.round((40*reg)**2)) # gives 2060 for our example\n",
    "    for f in range(28):\n",
    "        if (integer - f) % 27 == 0:\n",
    "            F = f\n",
    "    M = (integer - F)//27\n",
    "    return F, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['ps_ind_sum_bin'] = train[BIN].sum(axis = 1 )\n",
    "#train['ps_reg_mult'] = train.ps_reg_01 * train.ps_reg_02 * train.ps_reg_03\n",
    "#train['ps_car_13_ps_reg_03']= train.ps_car_13* train.ps_reg_03\n",
    "X_train['ps_car_15_carre'] = (X_train.ps_car_15)**2\n",
    "X_train['ps_ind_19_bin'] = X_train[['ps_ind_16_bin','ps_ind_17_bin','ps_ind_18_bin']].sum(axis = 1).apply(lambda x: 1 if x == 0 else 0)\n",
    "X_train['ps_reg_F'] = X_train['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "X_train['ps_reg_M'] = X_train['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "\n",
    "#test['ps_ind_sum_bin'] = test[BIN].sum(axis = 1 )\n",
    "#test['ps_reg_mult'] = test.ps_reg_01 * test.ps_reg_02 * test.ps_reg_03\n",
    "#test['ps_car_13_ps_reg_03']= test.ps_car_13* test.ps_reg_03\n",
    "X_test['ps_car_15_carre'] = (X_test.ps_car_15)**2\n",
    "X_test['ps_ind_19_bin'] = X_test[['ps_ind_16_bin','ps_ind_17_bin','ps_ind_18_bin']].sum(axis = 1).apply(lambda x: 1 if x == 0 else 0)\n",
    "X_test['ps_reg_F'] = X_test['ps_reg_03'].apply(lambda x: recon(x)[0])\n",
    "X_test['ps_reg_M'] = X_test['ps_reg_03'].apply(lambda x: recon(x)[1])\n",
    "\n",
    "drop_columns = ['ps_calc_10', 'ps_calc_01', 'ps_calc_04', 'ps_car_02_cat', 'ps_calc_14',\n",
    "               'ps_calc_08', 'ps_calc_17_bin', 'ps_car_10_cat', 'ps_ind_11_bin', 'ps_calc_12',\n",
    "               'ps_calc_09', 'ps_car_06_cat', 'ps_calc_05','ps_calc_16_bin', 'ps_calc_20_bin',\n",
    "                'ps_calc_18_bin']\n",
    "\n",
    "X_train = X_train.drop(drop_columns, axis=1)\n",
    "X_test = X_test.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_1 = pd.read_csv('save_meta/meta_train_1.csv', index_col= False)\n",
    "meta_train_2 = pd.read_csv('save_meta/meta_train_2.csv', index_col= False)\n",
    "meta_train_3 = pd.read_csv('save_meta/meta_train_3.csv', index_col= False)\n",
    "meta_train_4 = pd.read_csv('save_meta/meta_train_4.csv', index_col= False)\n",
    "\n",
    "meta_test_1 = pd.read_csv('save_meta/meta_test_1.csv', index_col= False)\n",
    "meta_test_2 = pd.read_csv('save_meta/meta_test_2.csv', index_col= False)\n",
    "meta_test_3 = pd.read_csv('save_meta/meta_test_3.csv', index_col= False)\n",
    "meta_test_4 = pd.read_csv('save_meta/meta_test_4.csv', index_col= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = pd.concat((X_train, meta_train_1, meta_train_2, meta_train_3), axis=1)\n",
    "X_test_3 = pd.concat((X_test, meta_test_1, meta_test_2, meta_test_3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(y_train, n_folds=5, shuffle=True, random_state=115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gini metric - from https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703#5897\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "    assert( len(actual) == len(pred) )\n",
    "    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "    totalLosses = all[:,0].sum()\n",
    "    giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    "    \n",
    "    giniSum -= (len(actual) + 1) / 2.\n",
    "    return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "# Create an XGBoost-compatible metric from Gini\n",
    "\n",
    "def gini_xgb(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    gini_score = gini_normalized(labels, preds)\n",
    "    return 'gini', gini_score\n",
    "    \n",
    "# We drop these variables as we don't want to train on them\n",
    "# The other 57 columns are all numerical and can be trained on without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5]\n",
      "[0]\ttrain-error:0.036437\tvalid-error:0.036444\ttrain-gini:0.278544\tvalid-gini:0.269835\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-error:0.036446\tvalid-error:0.036444\ttrain-gini:0.299351\tvalid-gini:0.283691\n",
      "Stopping. Best iteration:\n",
      "[52]\ttrain-error:0.036433\tvalid-error:0.036437\ttrain-gini:0.294394\tvalid-gini:0.283761\n",
      "\n",
      "[Fold 1/5 Prediciton:]\n",
      "[Fold 2/5]\n",
      "[0]\ttrain-error:0.03644\tvalid-error:0.036451\ttrain-gini:0.27671\tvalid-gini:0.272083\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-error:0.036449\tvalid-error:0.036444\ttrain-gini:0.298639\tvalid-gini:0.288787\n",
      "[200]\ttrain-error:0.036442\tvalid-error:0.036458\ttrain-gini:0.326282\tvalid-gini:0.288337\n",
      "Stopping. Best iteration:\n",
      "[116]\ttrain-error:0.036449\tvalid-error:0.036444\ttrain-gini:0.30217\tvalid-gini:0.289168\n",
      "\n",
      "[Fold 2/5 Prediciton:]\n",
      "[Fold 3/5]\n",
      "[0]\ttrain-error:0.036435\tvalid-error:0.036451\ttrain-gini:0.270814\tvalid-gini:0.277315\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-error:0.036444\tvalid-error:0.036451\ttrain-gini:0.297178\tvalid-gini:0.29369\n",
      "[200]\ttrain-error:0.036437\tvalid-error:0.036451\ttrain-gini:0.323153\tvalid-gini:0.293193\n",
      "Stopping. Best iteration:\n",
      "[139]\ttrain-error:0.036444\tvalid-error:0.036451\ttrain-gini:0.306674\tvalid-gini:0.294189\n",
      "\n",
      "[Fold 3/5 Prediciton:]\n",
      "[Fold 4/5]\n",
      "[0]\ttrain-error:0.036446\tvalid-error:0.036451\ttrain-gini:0.280675\tvalid-gini:0.284404\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-error:0.036446\tvalid-error:0.036451\ttrain-gini:0.296858\tvalid-gini:0.295012\n",
      "[200]\ttrain-error:0.036442\tvalid-error:0.036451\ttrain-gini:0.323719\tvalid-gini:0.293758\n",
      "Stopping. Best iteration:\n",
      "[102]\ttrain-error:0.036446\tvalid-error:0.036451\ttrain-gini:0.29748\tvalid-gini:0.295046\n",
      "\n",
      "[Fold 4/5 Prediciton:]\n",
      "[Fold 5/5]\n",
      "[0]\ttrain-error:0.036449\tvalid-error:0.036444\ttrain-gini:0.281568\tvalid-gini:0.281704\n",
      "Multiple eval metrics have been passed: 'valid-gini' will be used for early stopping.\n",
      "\n",
      "Will train until valid-gini hasn't improved in 100 rounds.\n",
      "[100]\ttrain-error:0.036449\tvalid-error:0.036444\ttrain-gini:0.296306\tvalid-gini:0.292273\n",
      "[200]\ttrain-error:0.036449\tvalid-error:0.036444\ttrain-gini:0.324089\tvalid-gini:0.291715\n",
      "Stopping. Best iteration:\n",
      "[132]\ttrain-error:0.036449\tvalid-error:0.036444\ttrain-gini:0.303388\tvalid-gini:0.292971\n",
      "\n",
      "[Fold 5/5 Prediciton:]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "y = y_train\n",
    "\n",
    "X = X_train_3\n",
    "x_test = X_test_3.values\n",
    "xgbscores = []\n",
    "\n",
    "# Create a submission file\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = np.zeros_like(id_test)\n",
    "\n",
    "# Set xgb parameters\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eta'] = 0.03\n",
    "params['gamma'] = 9\n",
    "params['silent'] = True\n",
    "params['max_depth'] = 6\n",
    "params['subsample'] = 0.9\n",
    "params['colsample_bytree'] = 0.85\n",
    "params['colsample_bylevel'] = 0.9\n",
    "params['tree_method'] = 'exact'\n",
    "\n",
    "# Take a random 30% of the dataset as validation data\n",
    "\n",
    "kfold = 5\n",
    "sss = StratifiedShuffleSplit(n_splits=kfold, test_size=0.25, random_state=0)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    X_train, X_valid = X.values[train_index], X.values[test_index]\n",
    "    y_train, y_valid = y[train_index], y[test_index]\n",
    "    # Convert our data into LGBoost format\n",
    "    d_train = xgb.DMatrix(X_train, y_train)\n",
    "    d_valid = xgb.DMatrix(X_valid, y_valid)\n",
    "    d_test = xgb.DMatrix(x_test)\n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "    # Train the model! We pass in a max of 2,000 rounds (with early stopping after 100)\n",
    "    # and the custom metric (maximize=True tells xgb that higher metric is better)\n",
    "    mdl = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=100, feval=gini_xgb, maximize=True, verbose_eval=100)\n",
    "\n",
    "    print('[Fold %d/%d Prediciton:]' % (i + 1, kfold))\n",
    "    # Predict on our test data\n",
    "    p_test = mdl.predict(d_test)\n",
    "    sub['target'] += p_test/kfold\n",
    "\n",
    "# Create a submission file\n",
    "sub.to_csv('stacking4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
